\documentclass[12pt,english]{article}
\usepackage{amsmath,amsfonts,amsthm,graphicx,mathtools,xcolor,parskip, gensymb}
%arg redefinitions
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\DeclarePairedDelimiter\norm{\lVert}{\rVert}

\begin{document}

draft; to be checked

\section{Stochastic MAB}

For \(i=1,\dots,K\), associate \(i\)-th bandit arm with an unknown probability distribution of \(X_{i,t} = \mu_i + \varepsilon_t\) where \(\mu_i\) is its mean value and \(\varepsilon_t \sim \mathcal{N}\left( 0, \sigma_2 \right) \). For a fixed \(i\) and any \(t \in \left\{ 1,\dots,T \right\} \), distribution of \(X_{i,t}\) is the same, i.e.\@ each arm keeps its distribution the whole time and we keep track of time index only to account for randomness due to \(\varepsilon_t\). 

Realization of \(X_{i,t}\) is called a reward \(r_{i,t}\), and a reward vector at each trial is \(\mathbf{r}_t = \left( r_{1,t},\dots,r_{i,t},\dots,r_{K,t} \right) \). Having pulled the \(i\)-th arm at \(t\), only the \(i\)-th component of \(\mathbf{r}_t\) is observed at \(t\) (no feedback from other guys at the casino!).

Set of actions \(\mathcal{A}=\left\{ 1,\dots,K \right\} \) are possible arms to pull. At each trial \(t\), \(a_{t} \in \mathcal{A}\) is choosen based on \(I_{t}\). The distribution of \(I_{t}\) is the policy that governs pulling arms, e.g.\@ \(I_{t} \sim \mathcal{U}\left\{ 1,K \right\} \) if a random arm is picked at \(t\). 

Let \(X_{I_t}\) be a distribution of an arm choosen according to \(I_t\). Having the policy dictate \(I_{t} = i\), the result of drawing from \(X_{I_t=i,t}\) is a received reward \(r_{i,t}\) in the reward vector \(\mathbf{r}_{t}\). The expected reward \(\mathbb{E} \left[  r_{i,t} \sim X_{It}\right] \) depends on \(I_t\) because the policy dictates the arm \(i\) inducing reward \(r_{i,t}\) generated by \(X_{i,t}\).

The goal is to \textbf{maximize the expected total reward} after \(T\) trials:

\begin{equation}
\max \mathbb{E}_{It} \left[ \sum_{t=1}^{T} r_{i,t} \right] 	
\end{equation}

Which would happen if the expected total reward were equal to the \textbf{optimal expected total reward} which is defined:

\begin{gather} \label{eq-optimal-rew}
\mathbb{E}_{It} \left[ \sum_{t=1}^{T} r_{i^{\ast},t} \right] = \mathbb{E}_{It} \left[ \sum_{t=1}^{T} \mu_{t}^{\ast} \right] \\
\mu_{t}^{\ast} = \max_{i \in \left\{ 1,\dots,K \right\} } \mu_{i}, \forall t
\end{gather}

Where \(r_{i^{\ast},t} = \mu_{t}^{\ast}\) is the optimal reward = highest expected mean obtained by choosing the optimal arm \(i^{\ast}\). The time indexing of \(r_{i^{\ast},t}, \mu_{t}^{\ast}\) is kept only for the sum, as the highest expected reward/mean remains constant throughout trails. There is a single best arm that ideally should be chosen right away and never switched from (impossible ideal \(I_{t}\) ). This task isn't straightforward due to the variance \(\varepsilon_t\) that clutters the true means of \(X_{i,t}\). The optimal expected total reward (\ref{eq-optimal-rew}) can be simplified as:
\begin{equation}
\mathbb{E}_{It} \left[ \sum_{t=1}^{T} \mu_{t}^{\ast} \right] = 	T \mu^{\ast}
\end{equation}

Instead of maximizing the expected total reward, we state an equivalent goal: \textbf{minimize the expected total regret \(R\left( T \right)\)}, where \(R\left( T \right) \) is defined as:
\begin{equation}\label{eq-r}
	\begin{aligned}
		R\left( T \right) &= \mathbb{E}_{I_t} \left[ \sum_{t=1}^{T} r_{i^{\ast},t} \right] - \mathbb{E}_{I_t} \left[ \sum_{t=1}^T r_{i,t}\right] \\
		&= \mathbb{E}_{I_t} \left[ \sum_{t=1}^{T} \mu_{t}^{\ast} \right] - \mathbb{E}_{I_t} \left[ \sum_{t=1}^T r_{i,t}\right] \\
		&= T\mu^{\ast} - \mathbb{E}_{I_t} \left[ \sum_{t=1}^T r_{i,t}\right]
	\end{aligned}
\end{equation}

We seek to minimize (\ref{eq-r}) by choosing an appropriate strategy \(I_{t}\), ideally choosing an optimal policy \(I_{t}^{\ast}\) that has a zero-regret \(R\left( T \right) = 0\).

We'll consider \(3\) policies:
\begin{itemize}
	\item Random policy \(I_{t}^{\mathcal{U}} \sim \mathcal{U}\left\{ 1,K \right\} \) 
	\item \(\varepsilon\)-greedy policy
		\[I_{t}^{\varepsilon} = \begin{cases}
		\argmax_{i} \hat{\mu}_{i,t} \quad \text{w/prob} \quad 1-\varepsilon\\
		I_{t}^{\mathcal{U}} \quad \text{w/prob} \quad \varepsilon
	\end{cases}\]\\ 
	where \(\hat{\mu}_{i,t}\) is the mean estimate of arm \(i\) at \(t\) and \(\varepsilon\) is\dots ? lookup, it seems \(\varepsilon\) is a tiny qunatity e.g.\@ \(.03\) which means exploit \(97\%\) of the time and explore during the remaining \(3\%\)  )
	\item Upper Confidence Bound (UCB) policy \dots todo 
\end{itemize}

\subsection{No variance case}
We assume no variance and thus \(X_{i,t} = \mu_i\) and \(r_{i,t} = \mu_{I_t}\). Total regret (\ref{eq-r}) becomes:
\begin{align}\label{eq-r-novar}
	R\left( T \right) &= \mathbb{E}_{I_t} \left[ \sum_{t=1}^{T} \mu_t^* - \sum_{t=1}^T \mu_{I_t} \right] \\
	R\left( T \right) &= T \mu^{\ast} - \sum_{t=1}^T \mu_{I_t}
\end{align}

In the epsilon-greedy policy, during exploitation we take the best known arm \(i\) that has \(\argmax_{i}	\hat{\mu}_{i,t}\). It's not an estimate but an exact mean of \(X_{i,t}\) because there is no variance. A single try on an arm gives perfect information on reward it gives.



\end{document}



